{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "sentences = [\n",
    "    'The RTX3080 is a marvel to behold.',\n",
    "    'This model\"s price has decreased compared to previous models.',\n",
    "    'The temperature of this model is a bit hot.',\n",
    "    'The cooling is honestly terrible.',\n",
    "    'The next installment in the RTX-series is one of a kind.',\n",
    "    'The RTX 3080 is a beast.'\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "stops"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "global dtn\n",
    "global adjectives\n",
    "adjectives = []\n",
    "global all_sentnc\n",
    "all_sentnc = []\n",
    "\n",
    "def processing(id, content):\n",
    "    global dtn\n",
    "    global adjectives\n",
    "    global all_sentnc\n",
    "\n",
    "    sentnc_list = []\n",
    "    sentnc_list_NN = []\n",
    "    sentnc_list_JJ = []\n",
    "\n",
    "    sentence_info = []\n",
    "    sentence_info.append(id)\n",
    "    sentence_info.append(content)\n",
    "\n",
    "    all_sentnc.append(sentence_info)\n",
    "    sentence_info = []\n",
    "\n",
    "    tokenized = nltk.word_tokenize(content)\n",
    "    tagged_list = nltk.pos_tag(tokenized)\n",
    "\n",
    "    for entry in tagged_list:\n",
    "        if \"JJ\" in entry:\n",
    "            sentnc_list_JJ.append(entry[0])\n",
    "        elif \"NN\" in entry or \"NNS\" in entry or \"NNP\" in entry or \"NNPS\" in entry:\n",
    "            sentnc_list_NN.append(entry[0])\n",
    "\n",
    "    sentnc_list.append(sentnc_list_NN)\n",
    "    sentnc_list.append(sentnc_list_JJ)\n",
    "\n",
    "    adjectives.append(sentnc_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "for i in range(len(sentences)):\n",
    "    processing(i, sentences[i])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "def getSentences():\n",
    "    return all_sentnc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "def getAspectTermDict():\n",
    "    return adjectives"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "aspect_term = getAspectTermDict()\n",
    "sentence_list = getSentences()\n",
    "\n",
    "aspect_terms = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "def detect_terms(temp, sentence, nnlist):\n",
    "    aspect_term = []\n",
    "    for relation in temp:\n",
    "        if relation[0] == 'amod':\n",
    "            aspect_term.append(relation[2].split('-')[0])\n",
    "        if (relation[0]=='acomp' or relation[0]=='xcomp' or relation[0]=='nmod' or relation[0]=='dobj'\n",
    "\t\tor relation[0]=='nsubj' or relation[0]=='nsubjpass' or relation[0]=='xcomp' \n",
    "\t\tor relation[0]=='pobj' or relation[0]=='abbrev'):\n",
    "\t\t        aspect_term.append(relation[4].split('-')[0])\n",
    "\n",
    "    aspect_term = list(set(aspect_term))\n",
    "    for term in aspect_term:\n",
    "        if term not in nnlist:\n",
    "            aspect_term.remove(temp)\n",
    "    for relation in temp:\n",
    "        if relation[0] == 'conj' and relation[2].split('-')[0] in aspect_term:\n",
    "            aspect_term.append(relation[4].split('-')[0])\n",
    "    aspect_term = list(set(aspect_term))\n",
    "    return aspect_term\n",
    "\n",
    "def findvblist(sentence):\n",
    "    sent = \"\"\n",
    "    token = sentence.split(\" \")\n",
    "    for word in token:\n",
    "        if word not in stops:\n",
    "            sent = sent+word\n",
    "            sent = sent+\" \"\n",
    "    sent.strip()\n",
    "    token = nltk.pos_tag(sent)\n",
    "    tagged_list = nltk.pos_tag(token)\n",
    "\n",
    "    vblist = []\n",
    "    for tags in tagged_list:\n",
    "        if (tags[1]=='VB' or tags[1]=='VBD' or tags[1]=='VBG' or tags[1]=='VBN' or tags[1]=='VBP'\n",
    "\t\tor tags[1]=='VBZ'):\n",
    "            vblist.append(tags[0])\n",
    "    return vblist\n",
    "\n",
    "def detect_quality(term, data, JJ_list, mydict, depth, key):\n",
    "    if depth == 2:\n",
    "        mydict[key] = list(set(mydict[key]))\n",
    "        return mydict\n",
    "    for relation in data:\n",
    "        if len(relation) >= 5:\n",
    "            if relation[2].split('-')[0] == term:\n",
    "                if relation[4].split('-')[0] in JJ_list:\n",
    "                    mydict[key].append(relation[4].split('-')[0])\n",
    "                detect_quality(relation[4].split('-')[0], data, JJ_list, mydict, depth+1, key)\n",
    "            elif relation[4].split('-')[0] == term:\n",
    "                if relation[2].split('-')[0] in JJ_list:\n",
    "                    mydict[key].append(relation[2].split('-')[0])\n",
    "                detect_quality(relation[2].split('-')[0], data, JJ_list, mydict, depth+1, key)\n",
    "    mydict[key] = list(set(mydict[key]))\n",
    "    return mydict\n",
    "\n",
    "for i in range(len(sentence_list)):\n",
    "    sentence = sentence_list[i][1]\n",
    "    pos_list = aspect_term[i]\n",
    "    temp = []\n",
    "    terms = []\n",
    "    os.popen(\"echo '\" + sentence + \"' > ./stanford-parser-full/data/sentence.txt\")\n",
    "    parser_out = os.popen(\"./stanford-parser-full/lexparser.sh ./stanford-parser-full/data/sentence.txt\")\n",
    "    for relation in parser_out:\n",
    "        relation = nltk.word_tokenize(relation)\n",
    "        if len(relation) != 0:\n",
    "            temp.append(relation)\n",
    "    x = detect_terms(temp, sentence, pos_list[0])\n",
    "    JJ_list = pos_list[1]\n",
    "    if len(JJ_list) == 0:\n",
    "        JJ_list = findvblist(sentence)\n",
    "    for term in x:\n",
    "        mydict = {}\n",
    "        mydict[term] = []\n",
    "        terms.append(detect_quality(term, temp, JJ_list, mydict, 0, term))\n",
    "    aspect_terms.append(terms)\n",
    "aspect_terms\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "tokens: expected a list of strings, got a string",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-5ee55022ce6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mJJ_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJJ_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mJJ_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindvblist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mmydict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-5ee55022ce6b>\u001b[0m in \u001b[0;36mfindvblist\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mtagged_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \"\"\"\n\u001b[1;32m    164\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# Throws Error if tokens is of string type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokens: expected a list of strings, got a string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tokens: expected a list of strings, got a string"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "235925901c488dc892d2f89dd3f7c0ff4d9c393ea687e852d9b2cae4ba30a691"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}